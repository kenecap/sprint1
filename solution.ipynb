{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4f1230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Чтение сырого файла: data/tweets.txt\n",
      "2. Очистка текста...\n",
      "Готово. Получено 1596801 чистых строк.\n",
      "4. Сохранение файлов...\n",
      "\n",
      "Процесс завершен! Файлы сохранены:\n",
      "  - Обучающая выборка: data/train.csv (1277440 строк)\n",
      "  - Валидационная выборка: data/val.csv (159680 строк)\n",
      "  - Тестовая выборка: data/test.csv (159681 строк)\n",
      "\n",
      "--- Пример данных из созданного train.csv ---\n",
      "                                                     text\n",
      "86449      or you could be like me work six days in a row\n",
      "989848                 good morning have a wonderful week\n",
      "869848  hope this works or a simple pic what a capacit...\n",
      "938814  just read the sweet message my best friend sen...\n",
      "75206   why is no one shopping am on my lunch isnt out...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.data_utils import load_and_clean_data\n",
    "\n",
    "data_dir = Path('./data/')\n",
    "raw_data_path = data_dir / 'tweets.txt'\n",
    "\n",
    "clean_df = load_and_clean_data(raw_data_path)\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(clean_df, test_size=0.2, random_state=42)\n",
    "\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "print(\"4. Сохранение файлов...\")\n",
    "train_path = data_dir / 'train.csv'\n",
    "val_path = data_dir / 'val.csv'\n",
    "test_path = data_dir / 'test.csv'\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"\\nПроцесс завершен! Файлы сохранены:\")\n",
    "print(f\"  - Обучающая выборка: {train_path} ({len(train_df)} строк)\")\n",
    "print(f\"  - Валидационная выборка: {val_path} ({len(val_df)} строк)\")\n",
    "print(f\"  - Тестовая выборка: {test_path} ({len(test_df)} строк)\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Пример данных из созданного train.csv ---\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44839c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 20000\n",
      "DataLoader'ы готовы.\n",
      "Размер батча (вход): torch.Size([128, 26])\n",
      "Размер батча (цель): torch.Size([128, 26])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from src.next_token_dataset import TextDataset, PadCollate\n",
    "\n",
    "data_dir = Path('./data/')\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = TextDataset(file_path=data_dir / 'train.csv')\n",
    "\n",
    "vocab = train_dataset.word2idx\n",
    "pad_idx = vocab[train_dataset.pad_token]\n",
    "vocab_size = train_dataset.vocab_size\n",
    "\n",
    "val_dataset = TextDataset(file_path=data_dir / 'val.csv', vocab=vocab)\n",
    "test_dataset = TextDataset(file_path=data_dir / 'test.csv', vocab=vocab)\n",
    "\n",
    "collate_fn = PadCollate(pad_idx=pad_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Размер словаря: {vocab_size}\")\n",
    "print(\"DataLoader'ы готовы.\")\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "print(f\"Размер батча (вход): {inputs.shape}\")\n",
    "print(f\"Размер батча (цель): {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6c2aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель создана и успешно обработала один батч данных.\n",
      "Размер входа: torch.Size([128, 26])\n",
      "Размер выхода (логитов): torch.Size([128, 26, 20000])\n",
      "Ожидаемый размер выхода: (128, 26, 20000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.lstm_model import LSTMModel\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LSTMModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout_prob=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "output = model(inputs)\n",
    "\n",
    "print(\"Модель создана и успешно обработала один батч данных.\")\n",
    "print(f\"Размер входа: {inputs.shape}\")\n",
    "print(f\"Размер выхода (логитов): {output.shape}\")\n",
    "print(f\"Ожидаемый размер выхода: ({BATCH_SIZE}, {inputs.shape[1]}, {vocab_size})\")\n",
    "\n",
    "\n",
    "idx2word = train_dataset.idx2word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efafeeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Training: 100%|██████████| 9980/9980 [08:49<00:00, 18.84it/s]\n",
      "Validation Loss: 100%|██████████| 1248/1248 [00:25<00:00, 48.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 | Time: 555.67s\n",
      "Train Loss: 5.3679\n",
      "Validation Loss: 5.0174\n",
      "Model saved to models/best_lstm_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9980/9980 [08:57<00:00, 18.56it/s]\n",
      "Validation Loss: 100%|██████████| 1248/1248 [00:25<00:00, 48.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5 | Time: 563.46s\n",
      "Train Loss: 4.9695\n",
      "Validation Loss: 4.8791\n",
      "Model saved to models/best_lstm_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9980/9980 [08:57<00:00, 18.55it/s]\n",
      "Validation Loss: 100%|██████████| 1248/1248 [00:25<00:00, 48.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5 | Time: 563.56s\n",
      "Train Loss: 4.8627\n",
      "Validation Loss: 4.8186\n",
      "Model saved to models/best_lstm_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9980/9980 [08:57<00:00, 18.57it/s]\n",
      "Validation Loss: 100%|██████████| 1248/1248 [00:25<00:00, 48.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5 | Time: 563.49s\n",
      "Train Loss: 4.8038\n",
      "Validation Loss: 4.7877\n",
      "Model saved to models/best_lstm_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9980/9980 [08:58<00:00, 18.52it/s]\n",
      "Validation Loss: 100%|██████████| 1248/1248 [00:26<00:00, 47.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5 | Time: 564.81s\n",
      "Train Loss: 4.7648\n",
      "Validation Loss: 4.7668\n",
      "Model saved to models/best_lstm_model.pth\n",
      "\n",
      "Calculating ROUGE on validation set with the best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.27kB [00:00, 11.1MB/s]\n",
      "Calculating ROUGE: 100%|██████████| 20/20 [00:06<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': np.float64(0.7341423209622171), 'rouge2': np.float64(0.6808170117078052), 'rougeL': np.float64(0.7341640330187014), 'rougeLsum': np.float64(0.7341614718383911)}\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from src.lstm_train import train_loop\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5 \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    device=device,\n",
    "    vocab=vocab,\n",
    "    idx2word=idx2word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b1a251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Примеры генерации текста ---\n",
      "PROMPT: 'i feel so'\n",
      "GENERATED: '<bos> i feel so bad for you'\n",
      "\n",
      "PROMPT: 'today is a'\n",
      "GENERATED: '<bos> today is a good day'\n",
      "\n",
      "PROMPT: 'i want to'\n",
      "GENERATED: '<bos> i want to go to the beach but i cant'\n",
      "\n",
      "PROMPT: 'what are you'\n",
      "GENERATED: '<bos> what are you doing today'\n",
      "\n",
      "PROMPT: 'this is the'\n",
      "GENERATED: '<bos> this is the first time i was in the <unk>'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_path = Path('./models/best_lstm_model.pth')\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "\n",
    "test_prompts = [\n",
    "    \"i feel so\",\n",
    "    \"today is a\",\n",
    "    \"i want to\",\n",
    "    \"what are you\",\n",
    "    \"this is the\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- Примеры генерации текста ---\")\n",
    "for prompt in test_prompts:\n",
    "    generated_text = model.generate(\n",
    "        start_seq=prompt,\n",
    "        max_len=15,\n",
    "        vocab=vocab,\n",
    "        idx2word=idx2word,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"PROMPT: '{prompt}'\")\n",
    "    print(f\"GENERATED: '{generated_text}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c19ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Оценка качества distilgpt2 на валидационной выборке ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Evaluating Transformer:   0%|          | 8/2560 [00:00<02:20, 18.20it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Evaluating Transformer: 100%|██████████| 2560/2560 [01:46<00:00, 24.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Scores для distilgpt2:\n",
      "{'rouge1': np.float64(0.6428079579346148), 'rouge2': np.float64(0.5871336061540489), 'rougeL': np.float64(0.6428515842464073), 'rougeLsum': np.float64(0.6422893195632884)}\n",
      "\n",
      "\n",
      "--- Примеры генерации текста с помощью distilgpt2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: 'i feel so'\n",
      "GENERATED: 'i feel so happy with the way I've been doing this.\n",
      "\n",
      "\n",
      "\"I feel like I've had my best friend's wedding present for the last 2 years,\" he said, \"but I'm not sure what's going on with the wedding. I'm not sure how I'll get that done.\"'\n",
      "\n",
      "PROMPT: 'today is a'\n",
      "GENERATED: 'today is a way to look at the social and economic issues associated with a change in the political and economic environment.\"'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: 'i want to'\n",
      "GENERATED: 'i want to know why we are here.‰\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\n",
      "\n",
      "PROMPT: 'what are you'\n",
      "GENERATED: 'what are you going to do to get that back in the court?”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is a great example of how the court is going to treat both sides of the argument.'\n",
      "\n",
      "PROMPT: 'this is the'\n",
      "GENERATED: 'this is the same as that. I have done a lot of research about the human brain. If you can find out more about human brain structure, please feel free to send me a message.\n",
      "\n",
      "This article is from the archive of our partner.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.eval_transformer_pipeline import evaluate_transformer\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"--- Оценка качества distilgpt2 на валидационной выборке ---\")\n",
    "val_file_path = data_dir / 'val.csv'\n",
    "transformer_rouge_scores = evaluate_transformer(val_file_path, device, limit_rows=2560)\n",
    "\n",
    "print(\"\\nROUGE Scores для distilgpt2:\")\n",
    "print(transformer_rouge_scores)\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Примеры генерации текста с помощью distilgpt2 ---\")\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "test_prompts = [\n",
    "    \"i feel so\",\n",
    "    \"today is a\",\n",
    "    \"i want to\",\n",
    "    \"what are you\",\n",
    "    \"this is the\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    result = generator(\n",
    "        prompt, \n",
    "        max_length=20,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "    print(f\"PROMPT: '{prompt}'\")\n",
    "    print(f\"GENERATED: '{result[0]['generated_text']}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d507ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск оценки модели distilgpt2 на валидационной выборке...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Evaluating Transformer: 100%|██████████| 1000/1000 [00:40<00:00, 24.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ROUGE метрики для distilgpt2 ---\n",
      "{'rouge1': np.float64(0.6449691258211186), 'rouge2': np.float64(0.5882963689757161), 'rougeL': np.float64(0.6443800144601144), 'rougeLsum': np.float64(0.6443285582174039)}\n"
     ]
    }
   ],
   "source": [
    "from src.eval_transformer_pipeline import evaluate_transformer\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "val_path = './data/val.csv'\n",
    "\n",
    "print(\"Запуск оценки модели distilgpt2 на валидационной выборке...\")\n",
    "\n",
    "transformer_rouge_scores = evaluate_transformer(val_path, device, limit_rows=1000)\n",
    "\n",
    "print(\"\\n--- ROUGE метрики для distilgpt2 ---\")\n",
    "print(transformer_rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d325724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Примеры генерации текста (distilgpt2) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:    'i feel so'\n",
      "GENERATED: 'i feel so much better.\n",
      "But as my friend, I've been to this place for the past few years. I've always wanted to make sure I didn't feel so bad for myself. I try to make sure I don't feel bad for myself.\n",
      "I do feel good for people who want me to be good. I try to take care of myself in a way that makes people happy.\n",
      "Let me know how you feel about yourself & how you feel about yourself.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:    'today is a'\n",
      "GENERATED: 'today is a new project that is going to bring back the old magic of life: the secret life of the past and the future.\n",
      "\n",
      "\n",
      "The project is a new project that is going to bring back the old magic of life: the secret life of the past and the future. The first time you were able to see the history of the past, you had to come back to the past and see how the present has changed.\n",
      "The final project will consist of the following:\n",
      "The present\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The history of the past\n",
      "The'\n",
      "\n",
      "PROMPT:    'i want to'\n",
      "GENERATED: 'i want to know what they would love to see in the future.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:    'what are you'\n",
      "GENERATED: 'what are you getting?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\n",
      "\n",
      "PROMPT:    'this is the'\n",
      "GENERATED: 'this is the only way to see if the user is the same person.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    device=0 if device == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "\n",
    "test_prompts = [\n",
    "    \"i feel so\",\n",
    "    \"today is a\",\n",
    "    \"i want to\",\n",
    "    \"what are you\",\n",
    "    \"this is the\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- Примеры генерации текста (distilgpt2) ---\")\n",
    "for prompt in test_prompts:\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=20, \n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50\n",
    "    )\n",
    "    generated_text = result[0]['generated_text']\n",
    "    print(f\"PROMPT:    '{prompt}'\")\n",
    "    print(f\"GENERATED: '{generated_text}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240716d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Финальная оценка лучшей модели (LSTM) на тестовом датасете ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE: 100%|██████████| 100/100 [00:35<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Итоговые ROUGE Scores на тестовой выборке:\n",
      "ROUGE-1: 0.7329\n",
      "ROUGE-2: 0.6801\n"
     ]
    }
   ],
   "source": [
    "from src.eval_lstm import calculate_rouge\n",
    "from pathlib import Path\n",
    "\n",
    "model_path = Path('./models/best_lstm_model.pth')\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "print(\"--- Финальная оценка лучшей модели (LSTM) на тестовом датасете ---\")\n",
    "\n",
    "\n",
    "final_rouge_scores = calculate_rouge(model, test_loader, vocab, idx2word, device, limit_batches=100)\n",
    "\n",
    "print(\"\\nИтоговые ROUGE Scores на тестовой выборке:\")\n",
    "print(f\"ROUGE-1: {final_rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {final_rouge_scores['rouge2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ac0c4",
   "metadata": {},
   "source": [
    "## Итоговые выводы\n",
    "1. Качество на конкретной задаче: Если цель максимально точно дополнять короткие посты в том же стиле, что и исходные данные, то легкая LSTM-модель является явным фаворитом. Она лучше понимает контекст и стиль датасета, что подтверждается и метриками, и ручным анализом.\n",
    "2. Общая способность к генерации: Модель distilgpt2, безусловно, является более мощной и \"креативной\". Она способна генерировать гораздо более сложный и разнообразный текст. Однако для данной бизнес-задачи эта сложность является избыточной и даже вредной, так как нарушает стилистику.\n",
    "3. Ресурсы и ограничения: Наша LSTM-модель обучалась за разумное время, она легкая и быстрая в работе. distilgpt2 требует значительно больше ресурсов (памяти, вычислительной мощности), что может стать проблемой при внедрении.\n",
    "\n",
    "Рекомендуется использовать разработанную и обученную LSTM-модель."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c91145",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
